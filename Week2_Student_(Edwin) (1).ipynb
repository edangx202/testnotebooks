{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Week 2 - Transformers ü§ñ: MLPs in Disguise** (~2 hrs total)"
      ],
      "metadata": {
        "id": "04PZJ_WLbmZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 0 - Setup (~1 min)\n",
        "Before you begin, please clone this notebook!\n",
        "\n",
        "**File > Save a copy in Drive**"
      ],
      "metadata": {
        "id": "-Vsd23TK6NCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import exception\n",
        "#@title Step 1: Mount drive\n",
        "#@markdown Run this cell. If prompted, press \"Connect to Google Drive\" and select your Google account.\n",
        "#@markdown Then, under the folder icon üìÅ on the left panel, you should see the folder **drive** appear.\n",
        "from google.colab import drive\n",
        "from IPython.display import display, Markdown, HTML\n",
        "import os, sys\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "try:\n",
        "  drive.mount('/content/drive', force_remount=False)\n",
        "  sys.path.append('/content/drive/MyDrive/DLE-Jun23/Projects')\n",
        "  os.chdir('/content/drive/MyDrive/Colab Notebooks/')\n",
        "  display(\"‚≠ê Mounted successfully!\")\n",
        "except:\n",
        "  display(HTML('<span style=\"color:red\">An error occurred. Try again!</span>'))\n"
      ],
      "metadata": {
        "id": "oxAUjGRSbuTu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f36a3a6e-5dd5-4318-a857-84f825ffb80c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'‚≠ê Mounted successfully!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#@title Step 2: Import packages\n",
        "!pip install gradio tiktoken transformers bertviz sentence_transformers\n",
        "!git clone https://github.com/kevinwu23/minGPT.git\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/minGPT')\n",
        "import gradio as gr\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from dle_utils.dle_utils import *\n",
        "\n",
        "import os\n",
        "import tiktoken\n",
        "from contextlib import nullcontext\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "import bertviz\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "id": "EZ4gFztrcAHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note: Accessing minGPT\n",
        "\n",
        "In this assignment, you'll be modifying files in a GitHub repo you have just cloned. This is located in\n",
        "\n",
        "`/content/drive/MyDrive/Colab Notebooks/minGPT`\n",
        "\n",
        "After you navigate to this folder, you can open files directly and modify them within Colab.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1v88T2um69mf4W4Jvzt2f-Se8-YlDDZmz\" width=300/>"
      ],
      "metadata": {
        "id": "bdUbql4zc-wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 0: Enter your name to begin.\n",
        "#@markdown Enter your name as it appears in Slack and run this cell! This is optional -- you can also leave this blank.\n",
        "Name = ' ' #@param {type:\"string\"}\n",
        "filepath = '/content/drive/MyDrive/Colab Notebooks/dle_info.txt'\n",
        "if os.path.exists(filepath):\n",
        "  print(\"Success!\")\n",
        "else:\n",
        "  if len(Name) == 0:\n",
        "    print(\"Please set your name!\")\n",
        "  else:\n",
        "    try:\n",
        "      with open(filepath, 'w') as fp:\n",
        "        fp.write(Name)\n",
        "        dle_username = Name\n",
        "      print(\"Success!\")\n",
        "    except:\n",
        "      print(\"Something went wrong...\")\n"
      ],
      "metadata": {
        "id": "Y03RSXQvcUjf",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15f0f75b-77b1-47cb-f655-0150b1d5ba80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: The Life of a Prompt (~10 min)\n",
        "\n",
        "<img src=\"https://seeklogo.com/images/C/chatgpt-logo-02AFA704B5-seeklogo.com.png\" width='200px' height='200px'>\n",
        "\n",
        "What happens when we type a prompt into ChatGPT? How does it know what to say? (How can we know it's not just a person pretending to be an AI on the other end? üòè).\n",
        "\n",
        "ChatGPT uses **GPT-3** as the backbone model for all of its responses. By backbone model, we mean that this is the model that interprets and aims to \"understand\" the prompt. There are other bells and whistles that are needed to make the experience smoother, but GPT-3 is the brains üß† behind everything.\n",
        "\n",
        "In this section will cover the basics of GPT (Generative Pre-trained Transformer). GPT was developed by OpenAI, and serves as the backbone of their flagship product, Chat-GPT.\n",
        "\n",
        "We will be working with a simplified version of GPT called minGPT (written by *Andrej Karpathy*) to be a more human-understandable implementation. Also, we'll be using GPT-2, which is a smaller, earlier version of GPT-3. Though different, there are actually very few conceptual differences between GPT-2 and GPT-3!\n",
        "\n",
        "**TODO**:\n",
        "\n",
        "Open up ```minGPT/mingpt/model.py``` in the folder tab on the left side of this notebook by double clicking its name. It should then appear on the right hand side of this notebook. The model we will be using is defined under ```class GPT(nn.Module)```.\n",
        "\n",
        "We'll dig into the model in a second. First, let's just run some examples through it to get a feel.\n",
        "\n",
        "**Run the cell below.** This cell loads the GPT-2 model and creates a function, `generate_text()`, that generates text using GPT-2 given a prompt."
      ],
      "metadata": {
        "id": "a_fF1tq1n2RC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from minGPT import mingpt\n",
        "from mingpt.model import GPT\n",
        "from mingpt.bpe import BPETokenizer\n",
        "\n",
        "model = GPT.from_pretrained('gpt2')\n",
        "tokenizer = BPETokenizer()\n",
        "\n",
        "def generate_text(prompt, model=model, num_samples=3, num_tokens_per_response=50):\n",
        "  x = tokenizer(prompt).expand(num_samples, -1)\n",
        "  response = model.generate(x, max_new_tokens=num_tokens_per_response, do_sample=True, top_k=5)\n",
        "  print(prompt)\n",
        "  print('-'*80)\n",
        "  for i in range(num_samples):\n",
        "    print(f'Response {i+1}: {tokenizer.decode(response[i].cpu().squeeze())[len(prompt):]}')\n",
        "    print('-'*80)"
      ],
      "metadata": {
        "id": "xw8cepvdoLqh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d5eb99b-42cf-4796-dc09-53dd8a91ba7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 124.44M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Try a prompt\n",
        "#@markdown In this cell, type in a prompt, starting with the default.\n",
        "Prompt = 'The best programming language for beginners is' #@param {type:\"string\"}\n",
        "\n",
        "generate_text(Prompt)"
      ],
      "metadata": {
        "id": "Zgb2Bu8sAn7R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f7d71d6-dc42-4c50-827b-416369aadec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best programming language for beginners is\n",
            "--------------------------------------------------------------------------------\n",
            "Response 1:  C++. C is the most popular programming language. C is a great way to learn and learn programming.\n",
            "\n",
            "The first part of C is C++. C is the most popular programming language. You can read C code in a a C\n",
            "--------------------------------------------------------------------------------\n",
            "Response 2:  Ruby.\n",
            "\n",
            "Ruby is the best programming language for beginners.\n",
            "\n",
            "Ruby's syntax is very readable and it's syntax is also very simple to read. It also has a lot of interesting properties.\n",
            "\n",
            "There is a lot of good information\n",
            "--------------------------------------------------------------------------------\n",
            "Response 3:  Python.\n",
            "\n",
            "The most important part of Python is that I've been writing Python for a long time. The reason why is simple.\n",
            "\n",
            "Python is a programming language. It has all these wonderful features. It is a language. It is\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéâ Hopefully, the answers from this model made some sense!\n",
        "\n",
        "If you find that the response is messy, this is in part because the model is spewing out words like a firehose. In practice, ChatGPT does additional tuning to make sure its responses are more natural.\n",
        "\n",
        "***So what is going on when a prompt is entered into the model?***\n",
        "\n",
        "We'll be going through a guided tutorial which we call **\"The life of a prompt\"**. We will take you through all the steps that a prompt goes through before the response is generated."
      ],
      "metadata": {
        "id": "nOVSTqsD_I46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Input preprocessing (~15 min)\n",
        "\n",
        "You'll notice in our function ```generate_text``` (the one provided a couple cells up, not in ```model.py```) that we do two steps before feeding the prompt into the model: (1) tokenization, (2) reshaping.\n",
        "\n",
        "(1) Tokenization is the process of breaking down free text into individual units (tokens). Tokens can be words like \"coffee\", \"cell\", or \"search\". They can also be subwords, such as the word \"psychology\" broken into \"psycho\" and \"logy\". This is what ```tokenize(prompt)``` does.\n",
        "\n",
        "In the cell below, we'll be using a tokenizer that's a little more understandable. (Read more about it [here](https://huggingface.co/transformers/v4.2.2/main_classes/tokenizer.html)).\n",
        "\n",
        "**TODO**: Run the line below to load the tokenizer.\n"
      ],
      "metadata": {
        "id": "ooR3-rNJ8Qod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "autotokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "oZC55D0MTP8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: Then, try a few different input strings to see how tokenization works:"
      ],
      "metadata": {
        "id": "KugMwyvjPall"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try changing input_string with a few of your own sentences!\n",
        "input_string = \"Ethiopian coffee tastes best with 10 minutes of brewing.\"\n",
        "\n",
        "autotokenizer.tokenize(input_string)"
      ],
      "metadata": {
        "id": "Z7sy0LAYTThZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41ce565e-61a7-406c-c1f8-3346f429e39e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ethiopian',\n",
              " 'coffee',\n",
              " 'tastes',\n",
              " 'best',\n",
              " 'with',\n",
              " '10',\n",
              " 'minutes',\n",
              " 'of',\n",
              " 'brewing',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you try a word like \"enumeration\", you'll notice the tokens are ['en', '##ume', '##ration']. The double hash \"##\" is used to indicate subwords that are part of larger words.\n",
        "\n",
        "This symbol is used by **Byte Pair Encoding (BPE)**, which is commonly used in NLP tasks. You'll also notice that it removes capitalization since it doesn't affect word meanings in most cases. (This is what the uncased means in `bert-base-uncased`).\n",
        "\n",
        "The next step is to convert tokens into input ids: Each token is mapped to an identification number in the overall vocabulary.\n",
        "\n",
        "**TODO**: In the cell below, find out what the input id for the word \"coffee\" is. Also, figure out what the size of the vocabulary is."
      ],
      "metadata": {
        "id": "QcbWDYVmUW8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You'll need the vocab first.\n",
        "vocab = autotokenizer.get_vocab()\n",
        "\n",
        "# Grab the input id for 'coffee'\n",
        "coffee_vocab_id = vocab['coffee']\n",
        "vocab_length = len(vocab)\n"
      ],
      "metadata": {
        "id": "HEV8lyGR0zyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check('2.1.1', tuple([coffee_vocab_id, vocab_length]))"
      ],
      "metadata": {
        "id": "Oq73COrzWT2y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "9501368e-0e56-4e9e-bfb2-ea2040184de7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Correct! üéâ"
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Easy enough, right? The tokenizer maps each prompt to its tokens, and then mapping those tokens to their input ids.\n",
        "\n",
        "Next, we are just left with reshaping the input.\n",
        "\n",
        "There's not too much to say about reshaping except for the fact that the line ```x = tokenizer(prompt).expand(num_samples, -1)``` has the ```expand``` component just to repeat the same prompt ```num_samples``` of times, each time aiming to get a different response to the prompt. (Think of it as a different \"try\" each time).\n",
        "\n",
        "___\n"
      ],
      "metadata": {
        "id": "KNGf5FNtXwag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Embeddings (~45 min)\n",
        "\n",
        "Scroll to line 283 in ```model.py```, where you'll find the function ```generate``` (different than the ```generate_text``` function we just worked with.) Try reading over this code to get a high-level understanding.\n",
        "\n",
        "The core of this function is found in line 293:\n",
        "\n",
        "```logits, _ = self(idx_cond)```.\n",
        "\n",
        "The self function calls ```forward```. In the ```forward``` function, you will find on line 267 our first focus point:\n",
        "\n",
        "```tok_emb = self.transformer.wte(idx)```\n",
        "\n",
        "Here, ```wte``` stands for word-token embedding.\n",
        "\n",
        "**TODO**: In the cell below, try running a sample prompt through this layer. Remember to first tokenize and reshape the prompt!\n",
        "\n"
      ],
      "metadata": {
        "id": "fCQRp6lZaDgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the prompt is tokenized and reshaped\n",
        "prompt = 'Into the rabbit hole we go'\n",
        "x = tokenizer(prompt)\n",
        "\n",
        "# Pass x through the model.transformer.wte function\n",
        "x_embedding = model.transformer.wte(x)\n",
        "\n",
        "print(x_embedding.size())"
      ],
      "metadata": {
        "id": "vcUacS5X02Xd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a2bb959-93e7-4a19-e94d-5d209b5b96c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 7, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do you notice about the size of ```x_embedding``` vs ```x```?\n",
        "What dimension has grown?\n"
      ],
      "metadata": {
        "id": "TU6kxlT9k084"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code defines ```model.transformer.wte```:\n",
        "\n",
        "```wte = nn.Embedding(config.vocab_size, config.n_embd)```\n",
        "\n",
        "An embedding layer is used, which you can learn more about [here](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html).\n",
        "\n",
        "**TODO**: To familiarize yourself with embedding layers, initialize an ```nn.Embedding``` layer with ```num_embeddings = 10``` and ```embedding_dim = 5```. Then, come up with an input, ```in_x```, that produces an output of size (2, 4, 5) after it's passed through this embedding layer.\n",
        "\n",
        "Here are a couple hints:\n",
        "1. ```num_embeddings``` refers to the highest number any index can be.\n",
        "2. Look into Pytorch tensor type ```torch.LongTensor```."
      ],
      "metadata": {
        "id": "M_oy9iCxgHzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = torch.randint(0, 10, size=(2,4))\n",
        "emb_layer = nn.Embedding(num_embeddings=10, embedding_dim=5)\n",
        "emb_layer(sample).size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zt1ePzTnWT9d",
        "outputId": "96ddc81f-06d7-49dc-ff4a-dbdf5bae32f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create embedding layer\n",
        "emb_layer = nn.Embedding(num_embeddings=10, embedding_dim=5)\n",
        "\n",
        "# Create input that produces output of size (2, 4, 5)\n",
        "in_x = torch.randint(0, 10, size=(2,4))\n",
        "\n",
        "out_x = emb_layer(in_x)\n",
        "out_x.size()"
      ],
      "metadata": {
        "id": "88bWKJG_4HZ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3043ebd5-279b-44b2-e2c6-88e385459e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check('2.1.2', out_x)"
      ],
      "metadata": {
        "id": "81NgnREMx3ma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb3ed622-3961-4adb-b045-45fbb394fc4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Correct! üéâ"
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! So what does this all mean?\n",
        "\n",
        "In the shape (2, 4, 5), the 2 refers to the number of samples. For each sample, there is a (4, 5) tensor that represents the prompt. The 4 refers to the length of each prompt (number of tokens), and the final dimension 5 refers to the fact that each token is represented by a length-5 vector.\n",
        "\n",
        "**In a nutshell**:\n",
        "\n",
        "*Token -> nn.Embedding -> vector of size ```embedding_dim```*\n",
        "\n",
        "```nn.Embedding``` is a trainable layer, meaning that as the model sees more data, the exact way in which it associates the embeddings to each token changes.\n",
        "\n",
        "Let's see an example of this.\n",
        "\n",
        "**TODO**:\n",
        "\n",
        "In the cell below, create your own ```nn.Embedding``` layer that has the same number of input and output dimensions as ```model.transformer.wte```. Hint: You can find this out by just printing the object itself out."
      ],
      "metadata": {
        "id": "tNFQZF25yXmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn.Embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XzKKqUobf95",
        "outputId": "af4e6f08-f027-4703-d749-7899405c41e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.nn.modules.sparse.Embedding"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.transformer.wte"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qK4XxJFbaI1X",
        "outputId": "71e016cb-d856-4b89-9c52-55cd6efc1ada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50257, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the embedding layer\n",
        "untrained_embedding = nn.Embedding(num_embeddings=50257, embedding_dim=768)"
      ],
      "metadata": {
        "id": "cHhgDPGH82fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check('2.1.3', untrained_embedding)"
      ],
      "metadata": {
        "id": "xgrFV_N084FX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be4783e9-38ae-4bef-c816-2237e779f947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Correct! üéâ"
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A great gutcheck for this is to ask yourself what the first dimension refers to in the context of the tokenizer.\n",
        "\n",
        "Next, let's take two similar words and find out how well it associates them together **before any training**.\n",
        "\n",
        "To measure the similarity between two embedding vectors, we'll be using a score called cosine similarity (which is ```F.cosine_similarity``` in PyTorch) to compare the embeddings. Very similar embeddings will have a cosine similarity closer to 1, and very dissimilar embeddings will have cosine similarity closer to 0. **Hint: You will need to use the ```dim``` argument to compare the right dimension (the one with 768).**\n",
        "\n",
        "Finally, populate ```similarity_untrained``` with the cosine similarity as a numpy float. **You'll need to turn a torch tensor into numpy for this!** (Also, think about which dimension actually has the answer you want if you're running into dimension issues).\n"
      ],
      "metadata": {
        "id": "vP0My4VV8wgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use tokenizer to tokenize 'dog' and 'canine'\n",
        "word1 = tokenizer('dog')\n",
        "word2 = tokenizer('canine')\n",
        "\n",
        "# Extract embeddings\n",
        "word1_emb = untrained_embedding(word1)\n",
        "word2_emb = untrained_embedding(word2)\n",
        "\n",
        "# Compute the cosine similarity between word1_emb and word2_emb\n",
        "similarity_untrained = F.cosine_similarity(word1_emb, word2_emb, dim=2)\n",
        "similarity_untrained = similarity_untrained.detach().numpy() # Convert to numpy float\n",
        "\n",
        "print(similarity_untrained)"
      ],
      "metadata": {
        "id": "N9W1rQW74j3m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdab16b1-a32c-4fbc-f7f2-74dfbd2f700e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.03579557 0.05311876]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( word2.size() )                                     # canine had been broken into two tokens\n",
        "\n",
        "print( tokenizer.decode(word2.squeeze()[0].expand(1)) )   # sub-token 1\n",
        "print( tokenizer.decode(word2.squeeze()[1].expand(1)) )   # sub-token 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddJmiFetpJAB",
        "outputId": "7a165b51-ce72-4625-e833-343c413def71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2])\n",
            "can\n",
            "ine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**:\n",
        "Next, you will be using the ```wte``` (word token embedding) layer from the model that has already been trained. The goal is to notice the difference before and after model training. The embedding produced after training should be much better than random."
      ],
      "metadata": {
        "id": "Ep9W-XJQGBwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract embeddings\n",
        "word1_emb = model.transformer.wte(word1)\n",
        "word2_emb = model.transformer.wte(word2)\n",
        "\n",
        "# Compute the cosine similarity between word1_emb and word2_emb\n",
        "similarity_trained = F.cosine_similarity(word1_emb, word2_emb, dim=2)\n",
        "similarity_trained = similarity_trained.detach().numpy()          # Convert to numpy float\n",
        "\n",
        "print(similarity_trained)"
      ],
      "metadata": {
        "id": "lT1kzDZH53rd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05ced9fa-86a6-4250-b5ac-cf506efa2de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.27271554 0.22237119]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check('2.1.4', tuple([similarity_untrained.squeeze()[0], similarity_trained.squeeze()[0]]))       # similarity between 'dog' and 'can'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyRgJS3zp5Hj",
        "outputId": "81aae272-fb51-41a7-83ca-5e1d222cd305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Correct! üéâ"
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check('2.1.4', tuple([similarity_untrained.squeeze()[1], similarity_trained.squeeze()[1]]))       # similarity between 'dog' and 'ine'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "6Xng2Hgmp9JV",
        "outputId": "3c13aacc-4ad4-42ce-835a-618c382a6174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Correct! üéâ"
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check('2.1.4', tuple([similarity_untrained, similarity_trained]))"
      ],
      "metadata": {
        "id": "3eW5V2HNG7fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! If done correctly, you should find that the random initialization has a very low similarity score between two related words (\"dog\", \"canine\").\n",
        "\n",
        "After training, the model's ```wte``` layer associates similar words more closely. Another way to look at this is that the model's embeddings represent \"meaning\" much more strongly."
      ],
      "metadata": {
        "id": "HNC24ORnHYA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try one more exercise here. Rather than find the embedding for a single word, can we extract the embedding for a whole sentence?\n",
        "\n",
        "Because the embedding layer will produce a 768-sized vector for each token, we will need to find a way to \"squash\" that into a fixed-length vector. One common method is just to take the average value along each of the 768 dimensions.\n",
        "\n",
        "**TODO**:\n",
        "In the cell below, tokenize the prompt and extract the embeddings. What are the dimensions of the embeddings? Compute the mean embedding."
      ],
      "metadata": {
        "id": "TlkMm6WY6Q8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Dogs are one of the most popular pets in the world.\"\n",
        "\n",
        "# Tokenize sentence and extract embedding from the trained layer\n",
        "sentence_tokenized = tokenizer(sentence)\n",
        "sentence_emb = model.transformer.wte(sentence_tokenized)\n",
        "print(sentence_emb.size())\n",
        "\n",
        "# Compute the mean of the layer across all words in the sentence.\n",
        "mean_emb = torch.mean(sentence_emb, dim=1, keepdim=True)\n",
        "print(mean_emb.size())\n",
        "mean_emb = mean_emb.squeeze(1)\n",
        "print(mean_emb.size())"
      ],
      "metadata": {
        "id": "YFptIxYN-aCQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edca2587-1a5e-4473-be90-ab0439e4bbc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 1, 768])\n",
            "torch.Size([1, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check('2.1.5', mean_emb)"
      ],
      "metadata": {
        "id": "JW2Myu8296eC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7bcacd2-8d58-4bda-acd0-ebc61a63214d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Correct! üéâ"
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test out how well this works on the task of comparing sentence. Below are three sentences. The first sentence is a linear regression formula, and the second and third are statements about dogs. The idea is that the cosine similarity between sentence B and sentence C should be higher than the cosine similarity between sentence A and B or A and C.\n",
        "\n",
        "**TODO**: Run the following cell. Then, compute the cosine similarities below (as numpy floats)."
      ],
      "metadata": {
        "id": "4A_6Yjfm6yyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentenceA = \"y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ‚Ä¶ + Œ≤‚Çöx‚Çö + Œµ\"\n",
        "sentenceB = \"Dogs are one of the most popular pets in the world.\"\n",
        "sentenceC = \"There are over 300 different breeds of dogs.\""
      ],
      "metadata": {
        "id": "S4kiUPpX3Sjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentenceA_tokenized = tokenizer(sentenceA)\n",
        "sentenceB_tokenized = tokenizer(sentenceB)\n",
        "sentenceC_tokenized = tokenizer(sentenceC)\n",
        "\n",
        "# Extract mean sentence embeddings for each sentence.\n",
        "embA = torch.mean( model.transformer.wte(sentenceA_tokenized), dim=1, keepdim=True)\n",
        "embB = torch.mean( model.transformer.wte(sentenceB_tokenized), dim=1, keepdim=True)\n",
        "embC = torch.mean( model.transformer.wte(sentenceC_tokenized), dim=1, keepdim=True)\n",
        "\n",
        "# Compute the cosine similarity for each pair of sentences and convert to a numpy float\n",
        "ABsim = F.cosine_similarity(embA, embB, dim=2)\n",
        "BCsim = F.cosine_similarity(embB, embC, dim=2)\n",
        "ACsim = F.cosine_similarity(embA, embC, dim=2)\n",
        "\n",
        "print(ABsim, BCsim, ACsim)"
      ],
      "metadata": {
        "id": "4Xshnlvy_T31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d89c677-5ca9-46f3-9507-ca7d5880c564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5132]], grad_fn=<SumBackward1>) tensor([[0.8453]], grad_fn=<SumBackward1>) tensor([[0.6010]], grad_fn=<SumBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check('2.1.6', tuple([ABsim, BCsim, ACsim]))"
      ],
      "metadata": {
        "id": "t1HXSUvu-_IR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71829404-0d18-462e-98ab-5620f0895f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Correct! üéâ"
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see if this works on a larger scale. In the variable ```SENTENCES``` is a list of 100 sentences. 90 of these sentences are about cats or dogs, while 10 are about statistics.\n",
        "\n",
        "\n",
        "**TODO**: Can you use sentence embeddings to figure out which 10 are about statistics **without looking at them**?\n",
        "\n",
        "This part is open-ended. In general terms, you should: (1) extract and compute the mean embeddings for each sentence. (2) Compare the embeddings with one another. For example, you could try a [clustering method](https://scikit-learn.org/stable/modules/clustering.html)!\n",
        "\n",
        "Once you finish, populate ```outlier_idx``` with a list of the indices (a numpy array) in ```SENTENCES``` which contain the statistics sentences."
      ],
      "metadata": {
        "id": "87NjfVJS74gJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "\n",
        "sentence_example = SENTENCES[0]\n",
        "\n",
        "SENTENCES_tokenized = [tokenizer(sent) for sent in SENTENCES]\n",
        "emb_pt_list = [torch.mean( model.transformer.wte(sent_tokenized), dim=1, keepdim=True) for sent_tokenized in SENTENCES_tokenized ]\n",
        "emb_np_list = [ emb_pt.detach().numpy().squeeze() for emb_pt in emb_pt_list ]\n",
        "print( f\"shape of each embedding: {emb_np_list[0].shape}\" )\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "# initialize Kmeans clusterer, and fit\n",
        "kmeans = KMeans(n_clusters=2, random_state=0).fit(emb_np_list)\n",
        "\n",
        "unique, counts = np.unique(kmeans.labels_, return_counts=True)\n",
        "print(np.asarray((unique, counts)).T)      # label 0 should be the ones about statistics. There are ten\n",
        "\n",
        "outlier_idx = np.where( kmeans.labels_ == 0 )\n",
        "print(type( outlier_idx ))\n",
        "outlier_idx = outlier_idx[0]"
      ],
      "metadata": {
        "id": "kMxeyyyPE4w5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c431039-5e05-42a5-b9e6-dae408963a24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of each embedding: (768,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0 10]\n",
            " [ 1 90]]\n",
            "<class 'tuple'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check('2.1.7', outlier_idx)"
      ],
      "metadata": {
        "id": "XwyS_Tj3FH1F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "2347b44f-105e-4d70-8913-be6414e6a5bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Correct! üéâ"
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will be covering the ```block```, which is referenced on line 148 and defined starting line 73:\n",
        "\n",
        "Each block consists of two parts: ```self.attn``` and ```self.mlpf```. We'll focus on ```self.attn```, as ```self.mlpf``` is conceptually just an MLP.\n",
        "\n",
        "The attention mechanism is the core of what makes a Transformer, a Transformer.\n"
      ],
      "metadata": {
        "id": "qZlD8CLwcwsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Self-Attention (~30 min)\n",
        "\n",
        "If you  examine ```model.transformer.h```, you will see a list of blocks, which each contain their own ```attn``` layer.\n",
        "\n",
        "Let's say we have a sentence: \"All the dogs bark at John for food\". Intuitively, we understand that the word \"bark\" refers to the dogs, rather than John. Does the model understand the same?\n",
        "\n",
        "We will be testing this out in a little experiment below.\n",
        "\n",
        "First, tokenize the sentence and store its embedding using ```wte``` (use only one sample).\n"
      ],
      "metadata": {
        "id": "BYl2teB8_vgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_string = 'All the dogs bark at John for food'\n",
        "\n",
        "# Tokenize and extract embeddings\n",
        "x = tokenizer(input_string)\n",
        "x_emb = model.transformer.wte(x)\n",
        "x_emb.size()"
      ],
      "metadata": {
        "id": "GCvGrl2wAjH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba11c7ab-0c7d-4758-f08c-2b86495c9ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**:\n",
        "\n",
        "Next, go to line 29 where ```CausalSelfAttention``` is defined.\n",
        "\n",
        "We want to extract the attentions directly to see what's going on. After line 64, add ```return att```. This should come after ```att = F.softmax(att, dim=-1)``` and before ```att = self.attn_dropout(att)```. Next, comment out line 63: ```att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))```. This is done so to save computation, but we want the whole attention matrix for interpretability purposes.\n",
        "\n",
        "Save ```model.py``` and return back to this notebook.\n",
        "\n",
        "Now, in the cell below, do the following steps:\n",
        "\n",
        "1. First, examine ```model.transformer.h```. You will notice a ```ModuleList``` containing 12 blocks. We'll only be using the first block in this list. Pass the tokenized sentence ```x_emb``` into this first block and extract the attention matrix for your sentence. The output should have shape 1x12x8x8, where 12 refers to the number of attention heads. (Note: the number of blocks is not the same as the number of attention heads, they just happen to be both 12!) Then, take the mean attention across all 12 attention heads so that you are left with a tensor of size 8x8. Hint: You can convert a tensor of dimension (1, 8, 8) to dimension (8, 8) by just calling ```.squeeze(0)``` on that tensor."
      ],
      "metadata": {
        "id": "P_ec_LeXsISu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_block = model.transformer.h[0]     # first Block from ModuleList"
      ],
      "metadata": {
        "id": "S9AxhNKaePM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5iXE4ZI_43uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ImZSciAu43ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CqBCo8R043n_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q2UUwETM43lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WeFhYwky43ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_mat = first_block.attn(x_emb)\n",
        "attn_mat.shape"
      ],
      "metadata": {
        "id": "6DsBkx_KyApY",
        "outputId": "206475c9-2ccb-4bf2-e9ce-c5818822304b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using only the first block, extract the attention matrix and take the mean over all 12 attention heads\n",
        "ln_1_output = model.transformer.h[0].ln_1(x_emb)\n",
        "print(ln_1_output.size())\n",
        "attn_mat_12heads = model.transformer.h[0].attn(ln_1_output)\n",
        "print(attn_mat_12heads.size())\n",
        "\n",
        "# attn_mat = torch.mean( model.transformer.h[0].attn(x_emb), dim=1, keepdim=True)\n",
        "attn_mat = torch.mean( attn_mat_12heads, dim=1, keepdim=True)\n",
        "attn_mat = attn_mat.squeeze()\n",
        "attn_mat.size()"
      ],
      "metadata": {
        "id": "2JHD2LUdDKdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bfbc91a-41f0-47c7-f58c-276226bf4772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 8, 768])\n",
            "torch.Size([1, 8, 768])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([768])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check('2.1.8', attn_mat)"
      ],
      "metadata": {
        "id": "1zYYJhNNOVhe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "190ea909-dc0c-4cf4-e4a5-2b1ee6c27917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Sorry, try again! ü§≠"
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. At this point, you can also visualize the attention matrix by using the line ```plt.imshow(attn_mat)```. Next, we're interested in what the word \"bark\" is paying attention to. For each 8x8 tensor, extract the row that corresponds to the word \"bark\" (think about what index this is from the prompt). You should be left with a tensor of length 8."
      ],
      "metadata": {
        "id": "6BEgp60zEGGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_mat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DAVdhmeGdZE",
        "outputId": "ac2945e0-9e87-4ead-e05b-38e90641c945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.3057e+00,  1.3415e-01,  3.8986e-01, -7.5600e-02, -2.7296e-02,\n",
              "        -2.8090e-02, -2.2265e+00,  5.5289e-01, -5.0004e-01,  2.4462e-02,\n",
              "         2.2890e+00,  6.4521e-02, -2.1586e-02, -6.4076e-03,  6.8166e-02,\n",
              "         5.7930e-02,  7.3684e-01, -1.8638e-02, -4.0084e-02,  1.1767e+00,\n",
              "         1.1828e-01, -8.0318e-02, -1.0802e-01,  9.7264e-01,  3.3313e-02,\n",
              "         2.0862e-02,  1.8530e+00, -2.9028e-02,  3.4468e-02, -1.1366e-01,\n",
              "        -1.2811e-01,  1.2434e-02,  1.4809e-01,  5.1100e-03, -5.7764e-02,\n",
              "        -5.4166e-01, -9.0812e-01, -8.9209e-02,  5.3927e-02,  1.4803e-01,\n",
              "        -7.0249e-03, -1.0178e-01, -1.9548e-02, -8.6218e-02, -1.0198e+00,\n",
              "        -8.8977e-02, -5.7018e-02, -6.6674e-02, -6.7396e-01, -6.9389e-01,\n",
              "         1.1265e-02, -2.4676e-02,  9.8285e-01,  5.0208e-02, -1.2137e-01,\n",
              "        -6.8958e-01, -1.2525e-01,  1.4181e-02,  3.6411e-02, -2.1846e-01,\n",
              "        -1.8333e+00, -1.0485e-02,  6.6455e-01,  5.8729e-01, -1.0997e+01,\n",
              "        -4.0201e-02, -3.7333e-02,  4.2038e-01, -2.7851e+00,  1.9706e-02,\n",
              "        -4.0838e-01, -5.8522e-01,  6.7509e-02, -1.0791e-01, -1.4345e-02,\n",
              "         1.0674e-01,  5.1781e-01,  1.3610e+00,  6.3713e-01, -8.0428e-02,\n",
              "        -1.0169e-01,  7.8964e-02, -4.2469e-02,  2.2554e-01, -1.4277e-01,\n",
              "         1.8344e+00, -1.7596e-01,  6.9065e+00, -1.9012e-02,  4.6034e-01,\n",
              "        -4.5825e-02, -1.8716e-02,  2.8896e-02, -5.2517e-03, -8.1677e-02,\n",
              "         5.1644e-02, -1.3517e-01,  8.0703e-01, -3.5141e-02, -8.9392e-01,\n",
              "         7.0006e-02, -1.5452e+00,  1.1476e+00,  2.0822e-01, -6.3701e-03,\n",
              "         1.0283e-01,  3.3007e-03, -9.5727e-01, -3.9131e-02,  6.3505e-02,\n",
              "         4.8285e-01, -2.4100e-02,  2.4586e-01, -8.6179e-03,  2.5188e-01,\n",
              "        -8.1805e-03, -2.9925e-01, -3.4091e-01,  2.8271e-02,  1.4355e+00,\n",
              "         1.0734e-01,  3.2208e-02,  1.7526e-02, -8.0613e-02,  5.8016e-02,\n",
              "        -8.7953e-02,  4.9915e-02, -3.3780e-03,  1.3164e+00,  1.1929e+00,\n",
              "        -7.6970e-02, -6.9209e-02, -1.3828e-01, -6.6482e-02, -2.1095e-02,\n",
              "         9.0227e-02,  5.8591e-02,  7.9869e-01,  1.2570e-01, -2.6005e-01,\n",
              "         8.2977e-02,  3.4735e-02,  1.4219e-02,  1.7794e-03, -6.9303e-02,\n",
              "        -2.6152e-01, -9.1779e-02, -4.9780e-02,  1.1439e-01,  4.1930e-01,\n",
              "         1.3275e-01, -6.3024e-01,  2.7637e+00, -1.1787e+00,  1.7202e-04,\n",
              "        -6.8670e-02, -6.4892e-02,  8.3433e-02,  1.1454e-01, -1.0075e-01,\n",
              "         1.6482e+00,  9.4980e-02,  6.8542e-03,  6.7276e-01, -4.3116e-03,\n",
              "         1.4535e-01, -1.0612e-01, -7.4842e-01,  8.7491e-02, -9.6522e-02,\n",
              "         5.0388e-02, -2.5034e-02,  6.5902e-02, -3.3507e-01,  5.4655e-01,\n",
              "        -6.1286e-01, -6.1740e-01, -3.0117e-02,  1.0688e+00, -2.3496e-02,\n",
              "         1.4807e-01,  6.7626e-02,  6.4301e-02,  1.0331e-01,  1.5028e-03,\n",
              "        -8.0847e-02,  1.1291e-01,  5.7869e-01, -6.0429e-02, -2.9663e-02,\n",
              "        -4.6177e-03, -1.8983e-02, -1.0427e-01, -3.6896e-02, -2.6449e-02,\n",
              "        -5.2452e+00, -1.0576e-01,  3.1659e-02, -9.3931e-02, -9.8812e-01,\n",
              "        -2.6709e-01,  9.3773e-02, -9.0939e-02,  1.2944e-01,  3.1177e-02,\n",
              "        -4.4083e-02, -1.6976e-01, -3.1668e-02, -4.9146e-01,  6.8995e-02,\n",
              "         7.7156e-02,  1.2331e-01,  4.7905e-02,  8.0601e-01,  8.7132e-02,\n",
              "        -1.5521e-01,  5.3970e-01, -1.2363e-01, -1.1710e+00, -9.1363e-02,\n",
              "        -4.0362e-02,  3.0477e-02,  1.0077e-01, -1.4526e-01, -1.6610e-01,\n",
              "        -1.1903e-02,  8.9389e-03,  4.5107e-02,  2.2675e-03,  1.7586e-02,\n",
              "        -3.9899e-02,  1.1539e+00, -1.8045e-02,  5.7908e-02,  4.5936e-02,\n",
              "         4.0658e-01, -9.2592e-02, -2.4580e-02,  4.3392e-01,  1.0625e-01,\n",
              "        -1.0294e-01,  3.0967e-01, -1.4594e-02, -9.4793e-02, -1.0186e-01,\n",
              "        -1.3602e-01,  9.2598e-03, -1.1196e+00, -5.2352e-02, -3.2747e-01,\n",
              "         2.6401e-02, -1.2224e+00, -6.0372e-02,  5.7277e-01,  8.2173e-02,\n",
              "        -1.1314e+00,  8.3449e-02,  1.7067e-01,  1.7726e-01, -8.3816e-01,\n",
              "        -1.2170e-01, -9.8682e-01, -6.5559e-02,  5.2768e-02, -8.0224e-02,\n",
              "        -8.0711e-02, -7.7884e+00,  6.7613e-01, -8.6326e-01,  5.7074e-01,\n",
              "         1.7754e-01, -5.0524e-01,  8.7641e-02,  1.4577e-01,  5.7088e-01,\n",
              "         9.9202e-02, -7.1415e-01, -1.6320e-02,  1.1380e-01,  6.8756e-02,\n",
              "        -7.5900e-02,  3.2031e-02, -1.7675e-01, -1.7586e+00,  5.7477e-01,\n",
              "         2.1952e-02, -1.8456e-01,  8.4478e-02,  8.9972e+00,  1.5649e+00,\n",
              "        -3.1015e-01, -7.1663e-03,  4.1999e-02,  8.8352e-02,  3.0746e-01,\n",
              "        -1.1423e-01,  1.9925e-01,  1.1316e-01, -6.3115e-02,  2.6716e-01,\n",
              "        -1.1334e+00, -1.3991e-01,  1.2501e-01,  5.4438e-02, -4.3602e-03,\n",
              "        -4.9498e-03, -5.5010e-02, -8.3763e-02, -1.4428e+00,  1.1564e-02,\n",
              "         7.4735e-02, -5.2471e+00,  5.1233e-02,  1.9760e-02,  2.5120e-03,\n",
              "         4.0827e-02, -2.2481e+00, -5.5381e-02, -1.5352e-02,  4.4489e-01,\n",
              "         3.1732e-01,  1.7833e-03,  2.3951e-02,  1.8563e+00, -1.2477e-01,\n",
              "        -1.0069e-01,  1.5771e+01, -4.4818e-02,  1.1415e+00,  4.1889e-02,\n",
              "        -5.0088e-02, -7.3082e-02,  1.3242e-02, -1.2514e-01, -9.2875e-02,\n",
              "         4.5613e-02,  2.1294e-01,  1.2592e-02, -3.1610e-02, -1.6894e+00,\n",
              "        -1.0186e-02,  6.4143e-02, -5.3298e-02,  2.4356e+00, -1.8476e-01,\n",
              "        -3.8082e-02, -1.2358e-01, -5.3518e-03,  2.4472e-02,  1.7717e-01,\n",
              "         1.9619e-01, -4.3038e+00, -8.3538e-02, -6.4149e-02,  1.3180e-01,\n",
              "         2.0048e-01, -2.2941e+00, -5.0373e-02, -1.7374e-01, -1.1711e-02,\n",
              "         6.4897e-01,  1.1060e-02, -3.4106e-02, -9.4652e-03,  7.0926e-02,\n",
              "         9.0983e-02,  4.0011e-04, -2.5082e-02,  5.2393e-02, -1.6681e-01,\n",
              "         5.3788e-01,  2.3607e-02, -3.7694e-02, -1.4324e+01,  5.2904e-02,\n",
              "         2.8192e-01, -5.5332e-01, -8.2490e-02, -1.2965e-01,  1.2779e+00,\n",
              "         6.6187e-03, -2.2196e-02, -1.0168e-01, -1.8754e+00, -9.5925e-02,\n",
              "         2.4929e+00,  7.2446e-01, -4.3465e-02,  5.7007e-02, -1.0094e+00,\n",
              "        -1.1323e-01, -1.0264e-01,  1.7027e-02,  1.3649e+01, -2.5079e-01,\n",
              "         1.2699e-01,  1.1213e-02, -6.0873e-02,  1.1345e-01,  2.0157e-02,\n",
              "        -3.6118e-02, -4.1889e-02, -4.7401e-01,  2.4938e-02, -6.6936e-03,\n",
              "         2.3736e-01, -8.8049e-02, -7.5320e-02, -1.1285e-01,  2.4058e-01,\n",
              "        -4.4214e-02,  5.1648e-02, -3.4392e-02, -2.8932e-02, -1.0164e+00,\n",
              "        -5.6398e-01,  4.0506e-02, -9.4791e-01,  2.9801e-01, -1.3904e-01,\n",
              "        -4.9336e-01, -1.4965e+00,  1.1818e-02,  1.9211e-01, -3.2021e-02,\n",
              "         8.0910e-02, -7.8776e-02,  1.6489e-01, -7.5950e-01,  1.1969e-01,\n",
              "        -1.4535e-02,  7.2803e-02, -1.9500e-01,  3.3285e-02,  6.0100e-02,\n",
              "         1.3727e-01, -8.0022e-02,  4.8059e-02, -4.3591e-01, -7.9690e-01,\n",
              "         2.8801e-02, -1.5658e-04, -2.7666e-01, -7.8229e-02,  5.0395e-01,\n",
              "        -5.4414e-02, -1.0044e-01,  1.4270e+01,  8.8773e-03,  1.7498e-02,\n",
              "        -2.6860e-01, -6.2743e-02,  1.0465e-01, -2.2160e-01,  1.3359e-01,\n",
              "         1.6944e-01, -4.9210e-01,  4.0416e-02, -5.6164e-02, -8.2081e-01,\n",
              "         1.6893e-02, -1.1663e+00,  2.3508e-02,  1.1666e-01, -7.4025e-01,\n",
              "         3.0343e-01,  1.5888e-01,  1.4866e-02, -4.7398e-02, -3.0615e-02,\n",
              "         9.0738e-02,  5.1760e-02, -3.4473e-01,  1.4360e-01,  4.2554e-02,\n",
              "         1.3715e-01,  4.3760e-03,  2.9695e-02,  3.5436e-02,  1.6833e-02,\n",
              "        -3.0440e+00, -2.2247e+00,  9.3936e-02,  7.6369e-02,  4.1864e-02,\n",
              "        -4.4212e-02, -4.6292e-03,  1.3713e-01, -1.2950e+00,  5.2835e-02,\n",
              "        -1.5084e-01,  4.1544e-01, -6.4739e-02,  8.5742e-02,  6.1737e-03,\n",
              "         9.8334e-02,  3.0887e-03, -1.0079e+00,  2.0583e-01, -1.0607e-01,\n",
              "        -1.7562e-02, -9.9816e-02,  8.5658e-01, -2.2522e+00,  1.3039e+00,\n",
              "        -4.2975e-02, -3.1840e-02, -1.2273e-01, -1.1103e-01,  4.5391e-02,\n",
              "         1.4653e-01,  4.2687e-01,  9.3978e-03, -4.2394e-02, -1.0931e-01,\n",
              "         3.0060e+00,  3.1498e-02,  1.4271e-01,  4.7017e-02,  8.1172e-02,\n",
              "         1.3142e-01, -1.0319e-02,  1.2842e-01, -5.3774e-02,  1.8370e-02,\n",
              "         4.5505e-03, -3.4591e+00,  8.9829e-01, -1.0122e+00,  2.8832e-02,\n",
              "         5.3216e-02,  1.6451e-03, -1.8532e-03,  1.9840e-02,  2.0759e-01,\n",
              "        -1.3169e-01, -1.8690e-01,  6.9596e-02,  3.7756e-01,  6.5103e-02,\n",
              "        -7.3417e-01, -1.3992e-01,  7.6686e-02,  7.7689e-02,  1.0739e-01,\n",
              "         1.1290e-01, -1.7046e-01, -1.2820e-02,  4.4796e-02, -1.3058e+00,\n",
              "        -6.1052e-02,  1.1542e-01,  6.2885e-02, -2.9211e-02, -2.0523e+00,\n",
              "         1.0358e-01,  1.0017e-01,  1.0271e-02, -1.0655e-01, -1.3988e-01,\n",
              "         2.4100e-01, -5.2452e-02, -6.3027e-02,  1.1437e-01,  3.7286e-02,\n",
              "         4.9281e-01, -9.8896e-01, -5.4207e-02,  8.3701e-01, -7.1195e-01,\n",
              "        -1.7189e-01, -7.7802e-01,  9.2965e-02, -8.0581e-02, -4.9865e-02,\n",
              "         2.3669e+00, -8.8317e-02,  1.2164e-01, -1.3328e+00,  9.8898e-02,\n",
              "         2.2365e-02,  6.4507e-02,  1.5222e-01,  1.2394e-01,  1.0108e-01,\n",
              "         5.6181e-02,  3.8106e-02,  1.2090e+00,  6.9916e-02,  2.1265e-01,\n",
              "        -1.4241e-01,  3.4253e-02, -5.8739e-02,  8.0638e-01, -5.8343e-02,\n",
              "        -5.8527e-02, -3.6618e-02, -3.9789e-02,  1.1438e-01,  5.4527e-02,\n",
              "        -2.5263e-01, -1.3899e-01,  1.2321e-01, -7.4033e-03, -1.1383e-01,\n",
              "        -1.4504e-02,  2.4090e-02,  1.0357e-01, -4.5746e-02,  8.0021e-02,\n",
              "         6.9232e-02, -6.9151e-03, -1.7237e-01, -1.3964e-01,  8.6960e-03,\n",
              "         4.6049e-01, -1.9287e-01, -2.5049e-02,  4.9314e-02,  6.0154e-01,\n",
              "        -3.0177e-01,  2.2354e-02, -9.1246e-01, -1.1638e-01, -2.2694e-01,\n",
              "        -8.5306e-02,  1.0663e-03,  1.1400e+00,  3.9949e-02,  2.1626e-02,\n",
              "         1.2175e-01,  9.9357e-02, -2.8841e-01,  4.4402e-02,  5.2276e-02,\n",
              "        -3.1429e+00, -2.0070e-01,  5.4209e-02, -8.1757e-03,  1.0240e-01,\n",
              "         3.0570e+00, -4.5437e-02,  9.7551e-01, -3.2646e-01,  6.2505e-02,\n",
              "         4.6286e-02, -2.3531e-03, -5.5443e-01,  1.9028e-01,  1.1719e+00,\n",
              "         5.4467e-02,  7.3902e-01,  2.8283e-02, -1.3717e-01,  1.9977e-01,\n",
              "        -1.4301e-01, -2.1887e-01, -5.2314e-01, -8.8066e-02,  1.7259e-01,\n",
              "        -6.7269e-02, -3.9514e-02,  1.4167e-01,  1.1603e-02,  6.8472e-02,\n",
              "        -6.6588e-01, -4.2833e-02,  5.1806e-01, -1.7620e-01, -1.1934e-01,\n",
              "        -1.0874e+00,  1.7888e-02, -6.0066e-02,  8.9918e-03,  1.7420e+00,\n",
              "         4.6505e-02, -1.1243e-01,  4.8204e-02,  2.0946e-02,  2.6682e+00,\n",
              "        -1.7477e-03,  3.3270e-02,  1.0034e-01,  6.9283e-02,  8.7759e-02,\n",
              "         9.1219e-01,  5.6680e-03, -4.7995e-02, -1.1747e-01,  7.4877e-02,\n",
              "        -6.1565e-02,  9.6262e-02, -4.7297e-02,  8.8326e-02, -1.2076e-02,\n",
              "        -6.3436e-02,  8.2867e-03,  2.5472e-02, -1.8602e-01, -3.2194e-01,\n",
              "         7.2499e-02, -4.3197e-01, -7.4058e-02,  9.9966e-01, -2.5397e-02,\n",
              "        -9.3252e-02, -2.4263e+00,  1.6273e-02,  2.5152e-01, -1.6947e-01,\n",
              "         1.9391e+00, -2.9545e-02, -1.1860e-01, -5.6711e-02, -9.8264e-02,\n",
              "        -1.4668e-01, -1.6094e-02,  2.2614e-01, -1.4602e+00,  8.5958e-02,\n",
              "        -1.3792e-02,  1.9089e+00, -4.8776e-02, -5.3753e-02, -1.2233e-01,\n",
              "        -3.0206e-01, -8.8657e-01,  5.7930e-01, -1.9842e-02,  2.7209e-02,\n",
              "         1.7166e-03, -1.4873e-01, -2.0644e-02, -1.5701e+00, -8.2239e-02,\n",
              "         2.9861e-01, -1.1566e-01,  7.6061e-02, -8.8799e-02, -1.9550e+00,\n",
              "        -2.3899e-01,  6.5236e-02,  1.9639e+00,  1.9147e-02,  1.0910e-01,\n",
              "        -2.7088e+00, -2.5310e+00,  1.4678e-01,  1.6502e-01,  1.8422e+00,\n",
              "        -9.4624e-02, -3.3666e-02, -1.3019e-01,  5.2285e-02,  1.4957e+00,\n",
              "         2.8681e+00, -1.3490e+00, -1.1800e+00,  6.0173e-03, -9.5274e-02,\n",
              "         1.5463e-02, -4.9712e-02, -6.7113e-02, -8.0063e-02, -1.5481e-02,\n",
              "        -2.7742e-02,  2.9180e-02, -1.3298e-01], grad_fn=<SqueezeBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to visualize what the attention matrix looks like\n",
        "plt.imshow(attn_mat.detach().numpy())"
      ],
      "metadata": {
        "id": "zjYCnt1dDNOg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "outputId": "fbcd949c-e675-483e-abd2-09dbb581d701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-60fe25b65b57>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run this cell to visualize what the attention matrix looks like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0minterpolation_stage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         resample=None, url=None, data=None, **kwargs):\n\u001b[0;32m-> 2695\u001b[0;31m     __ret = gca().imshow(\n\u001b[0m\u001b[1;32m   2696\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5663\u001b[0m                               **kwargs)\n\u001b[1;32m   5664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5665\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5666\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5667\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    708\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    709\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 710\u001b[0;31m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0m\u001b[1;32m    711\u001b[0m                             .format(self._A.shape))\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid shape (768,) for image data"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract just the row for the word 'bark'\n",
        "attn_row = attn_mat[3]"
      ],
      "metadata": {
        "id": "VWE7s7cROg9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check('2.1.9', attn_row)"
      ],
      "metadata": {
        "id": "1_PfU3gfOiNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Finally, return a dictionary that has each word in the sentence as a key and its attention score as the value (as a numpy float). It should look something like ```{'All': 0.01, 'the': 0.22, ...}```\n",
        "\n",
        "Which word does \"bark\" attend to the most (besides \"bark\")? Between the dog and John, which word does \"bark\" attend more strongly towards?"
      ],
      "metadata": {
        "id": "F0exj5szLexv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = tokenizer(input_string)\n",
        "print( f\"token_ids array shape:\\t {token_ids.squeeze().size()}\" )\n",
        "words = [ tokenizer.decode(token_id.expand(1)) for token_id in token_ids.squeeze() ]\n",
        "print(words)"
      ],
      "metadata": {
        "id": "VMx-6sZ2JB4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Populate dictionary of words and values\n",
        "# Remember to convert the attention scores to a numpy float!\n",
        "attn_dict = {}\n",
        "for i, word in enumerate(words):\n",
        "  attn_dict[word] = attn_mat[3, i].detach().numpy().item()\n",
        "\n",
        "print(attn_dict)"
      ],
      "metadata": {
        "id": "uV_HS0-mMnqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If this check isn't working, just make sure 'dog' has the highest score (next to 'bark')\n",
        "check('2.1.10', attn_dict)"
      ],
      "metadata": {
        "id": "AiGlPhuhO9jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try this for a sentence of your choice. (Sometimes, the tokenizer might split up a word into two pieces, so pick your sentences and attentions carefully)\n",
        "\n",
        "In the cell below, combine the code you wrote in the lines above and process the new input string to return the attention dictionary."
      ],
      "metadata": {
        "id": "qN2w2B3JLkiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attn_dict(sent, model, attn_row=0):\n",
        "  ln_1_output = model.transformer.h[0].ln_1(sent)\n",
        "  attn_mat_12heads = model.transformer.h[0].attn(ln_1_output)\n",
        "\n",
        "  attn_mat = torch.mean( model.transformer.h[0].attn(x_emb), dim=1, keepdim=True)\n",
        "  attn_mat = attn_mat.squeeze()\n",
        "\n",
        "  attn_dict = {}\n",
        "  for i, word in enumerate(words):\n",
        "    attn_dict[word] = attn_mat[attn_row, i].detach().numpy().item()\n",
        "\n",
        "  return attn_dict"
      ],
      "metadata": {
        "id": "uYuMYFqYMuXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_string = 'Too much sugar is not good for you.'\n",
        "\n",
        "attn_dict = get_attn_dict(input_string, model)\n",
        "print(attn_dict)"
      ],
      "metadata": {
        "id": "GDhsjLqnRZAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check('2.1.11', attn_dict)"
      ],
      "metadata": {
        "id": "iN3tpUJKXwo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don't worry if this part is a bit involved! Feel free to ask away on Slack. (Think through what each dimension is referring to at each step).\n",
        "\n",
        "Do the attention scores still make sense? It would be great to share your thoughts! (You can alter the block you are examining, or even the method of taking the average over all heads).\n",
        "\n",
        "At the end of the day, interpreting attention scores are a finicky business, so don't be discouraged if you aren't getting exactly what you're looking for!"
      ],
      "metadata": {
        "id": "kIYkhOngkJI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Outputs (~20 min)\n",
        "\n",
        "Together, the input preprocessing, embeddings, and self-attention are the key parts of a transformer. There are many other components that make it work, and we will conver some of them next week (and other parts, you can familiarize on your own).\n",
        "\n",
        "Finally, we'll focus on how the output is processed.\n",
        "\n",
        "Delete the ```return att``` line you've injected in script, and uncomment out line 63. Remember to save! Now, return to line 293: ```logits, _ = self(idx_cond)```. We can recreate this line below:"
      ],
      "metadata": {
        "id": "dTuTmES6AT4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Just run the line below!\n",
        "\n",
        "input_string = 'To make bread, first add'\n",
        "x = tokenizer(input_string)\n",
        "\n",
        "logits_orig, _ = model(x)"
      ],
      "metadata": {
        "id": "wP40C3o5xwR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examine the size of ```logits```. What does each dimension refer to?\n",
        "\n",
        "Next, we only want to know what the model's next word prediction is after the final word of our input. In the cell below, extract the logits from the last word."
      ],
      "metadata": {
        "id": "QlCvTtQPyAbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Just extract the last in the sequence. Think carefully about which dimension you are extracting from!\n",
        "logits = ..."
      ],
      "metadata": {
        "id": "dByhQIEPRQ_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check('2.1.12', logits)"
      ],
      "metadata": {
        "id": "bAey_PvFRjVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the cell below, apply the softmax layer to the logits. This makes it so that all the probabilities assigned to each vocab word sum to one."
      ],
      "metadata": {
        "id": "vqXEbxUJ0Qcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Just copy-paste line 301\n",
        "probs = ..."
      ],
      "metadata": {
        "id": "fmUSZSsVSBGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, compute the top ten predictions using ```torch.topk```. Then, populate a dictionary ```scores_dict``` where the keys are ten tokens and the values are their relative probabilities. The keys should be strings and the values should be numpy floats. You'll have to do some Pytorch tensor manipulation and data type conversions here! (Remember, use ```tokenizer.decode()``` to convert indices back into the words they refer to)"
      ],
      "metadata": {
        "id": "TtAaTQ9w1Ngk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract top 10 using torch.topk\n",
        "top10 = ...\n",
        "\n",
        "# Populate the dictionary\n",
        "scores_dict = ..."
      ],
      "metadata": {
        "id": "_m6CgwldTa37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check('2.1.13', scores_dict)"
      ],
      "metadata": {
        "id": "ehobAIsSS5Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll notice in the code, each logit is divided by a temperature. This smooths out the predictions such that there is less of a relative difference between the logits for each word.\n",
        "\n",
        "Let's see how this affects the relative predicted probabilities.\n",
        "Now, divide logits by a constant ```temperature=1.5```. Repeat the same steps in as the previous question and produce ```scores_dict_temp```, which is the scores after using temperature 1.5."
      ],
      "metadata": {
        "id": "y6vy6nga2lmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change logits by dividing it by 1.5\n",
        "logits = ...\n",
        "\n",
        "# Produce scores_dict again\n",
        "probs = ...\n",
        "top10 = ...\n",
        "scores_dict_temp = ..."
      ],
      "metadata": {
        "id": "rMuIMdkzUQMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check('2.1.14', tuple([scores_dict, scores_dict_temp]))"
      ],
      "metadata": {
        "id": "k2oDjs8ZUdo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that it does not change the relative order of probabilities.\n",
        "\n",
        "However, it will change their chance of being picked if we sample them at random.\n",
        "\n",
        "In the cell below, sample the next word using ```torch.multinomial```. After decoding the word, does the choice surprise you?\n",
        "\n",
        "Try running a few examples by changing ```num_samples``` to 10."
      ],
      "metadata": {
        "id": "jS0OeJEjznnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute next_word by using torch.multinomial\n",
        "next_word = ...\n",
        "# Decode and convert to string\n",
        "next_word = ...\n",
        "print(next_word)"
      ],
      "metadata": {
        "id": "g2b6V4QFVU1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll notice that by sampling, we get a more diverse choice of words, but perhaps more random.\n",
        "\n",
        "In general, this is a trade-off for large language models -- we want more creativity, but it risks non-sense responses!\n",
        "___\n",
        "\n",
        "## Summary\n",
        "\n",
        "Congrats! You got through the main parts of the GPT model!\n",
        "\n",
        "Feel free to continue playing around with prompts from the questions in this assignment and see if any responses surprise you!"
      ],
      "metadata": {
        "id": "43mgu2J6zseY"
      }
    }
  ]
}